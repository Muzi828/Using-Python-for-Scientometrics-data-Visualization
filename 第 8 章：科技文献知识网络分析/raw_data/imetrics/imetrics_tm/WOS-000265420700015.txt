This introductory article explores how the use of information affects the effectiveness of early warning systems. By effectiveness, we refer to the capacity of the system to detect and decide on the existence of a threat. There are two aspects to effectiveness: (a) being able to see the evidence that is indicative of a threat and (b) making the decision, based on the weight of the evidence, to warn that the threat exists. In early warning, information use is encumbered by cues that are fallible and equivocal. Cues that are true indicators of a threat are obscured in a cloud of events generated by chance. Moreover, policy makers face the difficult decision of whether to issue a warning based on the information received. Because the information is rarely complete or conclusive, such decisions have to consider the consequences of failing to warn or giving a false warning. We draw on sociocognitive theories of perception and judgment to analyze these two aspects of early warning: detection accuracy (How well does perception correspond to reality?) and decision sensitivity (How much evidence is needed to activate warning?) Using cognitive continuum theory, we examine how detection accuracy depends on the fit between the information needs profile of the threat and the information use environment of the warning system. Applying signal detection theory, we investigate how decision sensitivity depends on the assessment and balancing of the risks of misses and false alarms inherent in all early warning decision making.