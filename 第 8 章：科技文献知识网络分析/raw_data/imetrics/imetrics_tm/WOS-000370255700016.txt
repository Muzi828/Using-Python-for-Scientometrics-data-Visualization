Normalization of citation scores using reference sets based on Web of Science subject categories (WCs) has become an established (best) practice in evaluative bibliometrics. For example, the Times Higher Education World University Rankings are, among other things, based on this operationalization. However, WCs were developed decades ago for the purpose of information retrieval and evolved incrementally with the database; the classification is machine-based and partially manually corrected. Using the WC information science & library science and the WCs attributed to journals in the field of science and technology studies, we show that WCs do not provide sufficient analytical clarity to carry bibliometric normalization in evaluation practices because of indexer effects. Can the compliance with best practices be replaced with an ambition to develop best possible practices? New research questions can then be envisaged.