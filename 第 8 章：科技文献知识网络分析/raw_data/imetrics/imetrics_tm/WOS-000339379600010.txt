Although the nuclear era and the Cold War superpower competition have long since passed, governments are still investing in Big Science, although these large facilities are nowadays mostly geared towards areas of use closer to utility. Investments in Big Science are also motivated not only by promises of scientific breakthroughs but also by expectations (and demands) of measurable impact, and with an emerging global market of competing user-oriented Big Science facilities, quantitative measures of productivity and quality have become mainstream. Among these are rather simple and one-sided publication counts. This article uses publication counts and figures of expenditure for three cases that are disparate but all represent the state-of-the-art of Big Science of their times, discussing at depth the problems of using simple publication counts as a measure of performance in science. Showing, quite trivially, that Big Science is very expensive, the article also shows the absurd consequences of consistently using simple publication counts to display productivity and quality of Big Science, and concludes that such measures should be deemed irrelevant for analyses on the level of organizations in science and replaced by qualitative assessment of the content of the science produced.