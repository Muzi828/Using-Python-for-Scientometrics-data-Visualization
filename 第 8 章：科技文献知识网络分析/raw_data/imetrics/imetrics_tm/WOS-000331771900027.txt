In this paper, we show that an information source composed with n random variables may be split into 2(n) or 2(n) -1 "states"; therefore, one could compute the maximum entropy of the source. We derive the efficiency and the unused capacity of an information source. We demonstrate that in more than two dimensions, the transmission's variability depends on the system configuration; thus, we determine the upper and the lower bounds to the mutual information and propose the transmission power as an indicator of the Triple Helix of university-industry-government relationships. The transmission power is defined as the fraction of the total 'configurational information' produced in a system; it appears like the efficiency of the transmission and may be interpreted as the strength of the variables dependency, the strength of the synergy between the system's variable or the strength of information flow within the system. (C) 2013 Elsevier Ltd. All rights reserved.